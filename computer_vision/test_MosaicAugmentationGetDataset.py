# ********RoostGPT********
"""
Test generated by RoostGPT for test MiniProjects using AI Type  and AI Model 

ROOST_METHOD_HASH=get_dataset_f4ef937a5b
ROOST_METHOD_SIG_HASH=get_dataset_b2a4fcf575


Scenario 1: Successful extraction of image paths and labels
Details:
  TestName: test_successful_extraction_of_paths_and_labels
  Description: This test verifies that the function correctly extracts image paths and corresponding labels when provided with valid directories containing image annotations and images.
Execution:
  Arrange: Create a temporary directory structure with a set of .txt annotation files and corresponding .jpg image files.
  Act: Call the get_dataset function with the paths to the temporary label and image directories.
  Assert: Check that the returned image paths and labels match the expected list of image paths and the content of the annotation files.
Validation:
  The successful extraction of image paths and labels is crucial for any subsequent image processing tasks. Ensuring that the function returns the correct data validates that the function meets its primary business requirement to prepare data for further processing.

Scenario 2: Label directory with no annotation files
Details:
  TestName: test_label_directory_with_no_annotations
  Description: This test ensures the function can handle a scenario where the label directory does not contain any .txt annotation files.
Execution:
  Arrange: Create a temporary label directory without any .txt files and an image directory with .jpg files.
  Act: Call the get_dataset function with the paths to these directories.
  Assert: Confirm that the function returns empty lists for both image paths and labels.
Validation:
  This scenario validates the function's robustness in handling cases where the expected annotations are missing, which could occur in a real-world setting. The function should not fail but rather indicate that no data is available for processing.

Scenario 3: Image directory missing corresponding images
Details:
  TestName: test_image_directory_missing_corresponding_images
  Description: This test checks the function's behavior when the image directory does not contain images corresponding to the annotation files.
Execution:
  Arrange: Create a temporary label directory with .txt annotation files and an image directory without corresponding .jpg files.
  Act: Call the get_dataset function with the paths to these directories.
  Assert: Verify that the function returns lists with only the data that has corresponding image files.
Validation:
  It's important to ensure that the function does not include paths or labels for images that do not exist, as this would lead to errors in downstream processing. This test confirms that the function can handle discrepancies in the availability of corresponding image files.

Scenario 4: Incorrectly formatted annotation files
Details:
  TestName: test_incorrectly_formatted_annotation_files
  Description: This test ensures that the function can handle annotation files that are incorrectly formatted or contain invalid data.
Execution:
  Arrange: Create a temporary label directory with .txt files that have incorrect formatting or invalid data and an image directory with corresponding .jpg files.
  Act: Call the get_dataset function with the paths to these directories.
  Assert: Verify that the function skips the incorrectly formatted annotations and only returns valid image paths and labels.
Validation:
  The function should be resilient to errors in the annotation files. This test checks the function's ability to avoid processing invalid data, which is essential for maintaining data integrity in the dataset preparation process.

Scenario 5: Annotation files with no bounding boxes
Details:
  TestName: test_annotation_files_with_no_bounding_boxes
  Description: This test verifies that the function can handle annotation files that do not contain any bounding boxes (i.e., empty or with only newline characters).
Execution:
  Arrange: Create a temporary label directory with empty .txt files and an image directory with corresponding .jpg files.
  Act: Call the get_dataset function with the paths to these directories.
  Assert: Check that the function returns image paths and labels, excluding the ones with empty annotation files.
Validation:
  It is necessary to verify that the function can process annotation files that contain no bounding boxes, as this is a valid scenario that may occur in datasets. The function should not fail or include empty data in the output.

Scenario 6: Annotation files with negative or zero bounding box values
Details:
  TestName: test_annotation_files_with_negative_or_zero_values
  Description: This test checks whether the function correctly handles annotation files that contain bounding boxes with negative or zero values, which are invalid.
Execution:
  Arrange: Create a temporary label directory with .txt files containing bounding boxes with negative or zero values and an image directory with corresponding .jpg files.
  Act: Call the get_dataset function with the paths to these directories.
  Assert: Verify that the function excludes bounding boxes with invalid values and only includes valid ones in the labels list.
Validation:
  The test ensures that the function can filter out invalid bounding box values, which is important for the accuracy of the dataset and any further image processing tasks that rely on valid annotations.
"""

# ********RoostGPT********
import pytest
import tempfile
import shutil
import os
from computer_vision.mosaic_augmentation import get_dataset
import glob

class Test_MosaicAugmentationGetDataset:
    
    @pytest.fixture
    def setup_directories_with_files(self):
        with tempfile.TemporaryDirectory() as label_dir, tempfile.TemporaryDirectory() as img_dir:
            yield label_dir, img_dir

    @pytest.fixture
    def create_annotation_files(self, setup_directories_with_files, request):
        label_dir, img_dir = setup_directories_with_files
        annotations = request.param['annotations']
        for filename, content in annotations.items():
            label_path = os.path.join(label_dir, f"{filename}.txt")
            with open(label_path, 'w') as file:
                file.write(content)
            img_path = os.path.join(img_dir, f"{filename}.jpg")
            with open(img_path, 'w') as file:
                file.write('fake-image-data')
        return label_dir, img_dir

    @pytest.mark.valid
    @pytest.mark.smoke
    @pytest.mark.parametrize('create_annotation_files', [
        {'annotations': {'image1': '0 0.5 0.5 1 1\n1 0.5 0.5 1 1'}}
    ], indirect=True)
    def test_successful_extraction_of_paths_and_labels(self, create_annotation_files):
        label_dir, img_dir = create_annotation_files
        img_paths, labels = get_dataset(label_dir, img_dir)
        
        expected_img_paths = [os.path.join(img_dir, 'image1.jpg')]
        expected_labels = [[[0, 0.0, 0.0, 1.0, 1.0], [1, 0.0, 0.0, 1.0, 1.0]]]
        
        assert img_paths == expected_img_paths
        assert labels == expected_labels

    @pytest.mark.valid
    @pytest.mark.negative
    def test_label_directory_with_no_annotations(self, setup_directories_with_files):
        label_dir, img_dir = setup_directories_with_files
        # Create only image files
        for i in range(3):
            img_path = os.path.join(img_dir, f"image{i}.jpg")
            with open(img_path, 'w') as file:
                file.write('fake-image-data')
        img_paths, labels = get_dataset(label_dir, img_dir)
        assert img_paths == []
        assert labels == []

    @pytest.mark.valid
    @pytest.mark.negative
    def test_image_directory_missing_corresponding_images(self, setup_directories_with_files):
        label_dir, img_dir = setup_directories_with_files
        # Create only label files
        for i in range(3):
            label_path = os.path.join(label_dir, f"image{i}.txt")
            with open(label_path, 'w') as file:
                file.write('0 0.5 0.5 1 1\n')
        img_paths, labels = get_dataset(label_dir, img_dir)
        assert img_paths == []
        assert labels == []

    @pytest.mark.invalid
    @pytest.mark.parametrize('create_annotation_files', [
        {'annotations': {'image1': 'invalid data'}}
    ], indirect=True)
    def test_incorrectly_formatted_annotation_files(self, create_annotation_files):
        label_dir, img_dir = create_annotation_files
        img_paths, labels = get_dataset(label_dir, img_dir)
        assert img_paths == []
        assert labels == []

    @pytest.mark.valid
    @pytest.mark.negative
    @pytest.mark.parametrize('create_annotation_files', [
        {'annotations': {'image1': ''}}
    ], indirect=True)
    def test_annotation_files_with_no_bounding_boxes(self, create_annotation_files):
        label_dir, img_dir = create_annotation_files
        img_paths, labels = get_dataset(label_dir, img_dir)
        assert img_paths == []
        assert labels == []

    @pytest.mark.invalid
    @pytest.mark.parametrize('create_annotation_files', [
        {'annotations': {'image1': '0 -0.5 -0.5 0 0'}}
    ], indirect=True)
    def test_annotation_files_with_negative_or_zero_values(self, create_annotation_files):
        label_dir, img_dir = create_annotation_files
        img_paths, labels = get_dataset(label_dir, img_dir)
        # The test expects that invalid bounding boxes are excluded
        assert img_paths == [os.path.join(img_dir, 'image1.jpg')]
        assert labels == [[]] # No valid boxes, so an empty list is expected
